# SVM(Hard Margin) vs Soft Margin SVM

Changhyun Kim
2022-10-25

### 1. SVM에서 Soft and Hard Margin이란?

기본적으로 SVM은 선형으로 분리(Linearly Separable)된다고 가정한다.

하지만, 현실에서 선형으로 완벽히 분리가 되는 데이터는 찾아보기 어렵다.
따라서 SVM은 데이터의 오분류(error)를 허용해준다.

그렇지만 무분별하게 오분류를 허용하게 되면 classifier로써의 역할은
사라지게 때문에, SVM에서는 $C$ 라는 하이퍼파라미터를 통해 error를 얼마나
허용해 줄 것인지 정해준다. (하단의 수식 참고)

이때 error를 허용해주지 않는 경우를 Hard Margin SVM이라고 하는데 이는
사실 데이터를 선형으로 분리하는 SVM과 같다.

다음으로, error를 $C$값에 따라 어느정도 허용해주는 SVM은 Soft Margin SVM
이라고 한다.

해당 듀토리얼에서는 $C$ 값에 따른 SVM의 종류 (Hard Margin, Soft
Margin)을 알아보고, C 값에 따른 Soft Margin SVM의 변화를 알아보고자
한다.

또한 Hard Margin SVM과 Soft Margin SVM의 실제 Support Vector가 어떻게
생성되는지 알아보고자 한다.

### 2. Hard Margin SVM(Lineary Separable SVM)

일반 SVM(Hard Margin SVM) 모델의 랑그랑지안 Primal 식과 Dual 식은 아래와
같다.

<img src="image/hardmarginsvm.PNG" data-fig.align="center" />

### 3. Soft Margin SVM(SVM with Error)

Soft Margin SVM 모델의 랑그랑지안 Primal 식과 Dual 식은 아래와 같다.

<img src="image/softmarginsvm.PNG" data-fig.align="center" />

해당 식에는 위(Hard Margin)에는 식에는 없는 $C$ 가 존재하는데, 이는 뒤에
error($\epsilon_i$)를 얼마나 식에 반영할지 정하는 하이퍼 파라미터이다.

그런데, Hard Margin SVM과 Soft Margin SVM의 Dual식의 제약조건을 보면,

Soft Margin SVM에서 $a_i$가 $c$ 이하가 된다는 조건 이외에 두 식이 **모두
동일**함을 알 수 있다.

또한, 만약 Soft Margin SVM의 $c$가 $\infty$로 커지면 $c$ 이하가된다는
조건은 무의미해짐을 알 수 있다.

이런 경우 수식 상 Hard Margin과 Soft Margin SVM이 완벽히 같아짐을 알 수
있다.

고 만약 $c$가 매우 작아지면(0) error($\epsilon_i$)의 영향이 매우
작아지기 때문에 모든 error를 허용할 것이라고 예상할 수 있다.

(primal은 minimize 문제이기 때문에 $+C$가 0이되면 error의 영향이
무의미해짐 )

따라서 이후에는 다음과 같은 방법을 따라 실제로 Soft Margin SVM의 $c$가
$\infty$로 커지면 정말 Hard Margin SVM과 같아지는지 확인하고, $c$가 매우
작은 경우의 결과도 확인해 볼 예정이다.

1.  Linearly Saparable한 데이터를 생성한다.
2.  Soft Margin SVM의 $C$ 값이 아주 큰 경우($\infinity$)를 설정하고
    데이터를 분리한다.
3.  Soft Margin이지만 $C$가 아주 크게 되면 이론상 Hard Margin과
    같아짐으로 데이터를 선형분리 할 수 있다.
4.  이때의 $w$ 값을 알아본다.
5.  추가적으로 $c$가 매우 작은 경우를 설정하고
6.  이때의 $w$ 값을 알아본다.

### Linearly Saparable한 데이터를 생성한다.

```r
library(MASS)
library(e1071)
```

```r
n1 <- n2 <- 50 # 데이터 50개
m1 <- c(1.5,1) # 다변수정규분포1 평균
m2 <- c(-2,-2) # 다변수정규분포2 평균

v1 <- v2 <- diag(2) # 0,1 설정
x1 <- mvrnorm(n1, m1, v1) # 다변수정규분포1
x2 <- mvrnorm(n2, m2, v2) # 다변수정규분포2

# linearly separable한 데이터 생성
plot(x1[,1],x1[,2],col=2,xlim=c(-4,4),ylim=c(-4,4)) # x1 data plot
points(x2[,1],x2[,2],col=3) # x2 data plot
```

![](Hard-Margin-SVM-vs-Soft-Margin-SVM_files/figure-gfm/unnamed-chunk-1-1.png)<!-- -->

### 생성된 데이터 확인

```r
# dataframe check
dat <- data.frame(cbind(rep(c(0,1),each=n1),rbind(x1,x2))) # x1=0, x2=1
colnames(dat) <- c("y","z1","z2")
dat
```

    ##     y         z1          z2
    ## 1   0  1.9188422  1.59006107
    ## 2   0  1.8221929  1.55831998
    ## 3   0  2.5732373  0.79555287
    ## 4   0  1.2052996  1.93482192
    ## 5   0  1.1648423  0.71490695
    ## 6   0  0.9888044  0.79917918
    ## 7   0  0.6027445 -0.02016742
    ## 8   0  1.9362595  1.05370132
    ## 9   0  0.7851138  0.93075487
    ## 10  0  1.1365315  0.70212086
    ## 11  0  0.3799193  2.11751225
    ## 12  0  1.4952533  1.30864529
    ## 13  0  0.9863844  0.63267693
    ## 14  0  1.3703559 -0.09986815
    ## 15  0  0.7256311  0.76266372
    ## 16  0  1.6920683  0.71222302
    ## 17  0  2.0793062  1.44202318
    ## 18  0  0.3269874  2.16118753
    ## 19  0  2.7346929  2.01288568
    ## 20  0  2.3512057  1.45558624
    ## 21  0  1.5629074 -0.12238005
    ## 22  0 -0.8985827  1.29360210
    ## 23  0 -0.4796499  0.06098670
    ## 24  0  1.6794215  0.95206062
    ## 25  0  2.2182071 -0.16075293
    ## 26  0  1.8462524  0.64548573
    ## 27  0  0.7158111  1.89119823
    ## 28  0  1.4972269  0.21465836
    ## 29  0  0.9592400  0.83876885
    ## 30  0  2.0341461  2.26437259
    ## 31  0  0.4635967  1.05407295
    ## 32  0  1.2229281 -0.46766935
    ## 33  0  2.1045200 -0.11495100
    ## 34  0  2.1006125  1.56949341
    ## 35  0  2.7069272  0.91689016
    ## 36  0  2.3290342  0.99184179
    ## 37  0  2.3301967  0.76759119
    ## 38  0  3.1491198  2.31649956
    ## 39  0  1.3330660 -0.03958480
    ## 40  0  0.6516485  0.28359808
    ## 41  0  1.0242426 -0.18737857
    ## 42  0  2.6015815  1.74848551
    ## 43  0  1.9533956  0.87735918
    ## 44  0  1.1392820 -1.82312395
    ## 45  0  1.1626329  0.97149504
    ## 46  0  1.2552574  1.12442942
    ## 47  0  3.4094738  1.44007563
    ## 48  0  2.0721263  1.49953706
    ## 49  0  2.5711985  0.57610876
    ## 50  0  1.7470138  2.32561336
    ## 51  1 -1.3281299 -2.40832993
    ## 52  1 -1.7971735 -1.89786830
    ## 53  1 -2.5753811 -1.97482042
    ## 54  1 -0.5453427 -0.06499693
    ## 55  1 -3.5579687 -3.59451486
    ## 56  1 -0.8048273 -2.28068058
    ## 57  1 -1.3161074 -0.85460135
    ## 58  1 -3.2670197 -1.87537815
    ## 59  1 -1.8689411 -3.34060283
    ## 60  1 -2.9468116 -4.53376409
    ## 61  1 -1.5661167 -3.73983891
    ## 62  1 -0.7053250 -2.36305371
    ## 63  1 -1.1063913 -0.52472215
    ## 64  1 -1.1042534 -0.18242878
    ## 65  1 -2.6654957 -2.92552859
    ## 66  1 -3.6521907 -3.17358685
    ## 67  1 -1.6516443 -2.16360210
    ## 68  1 -0.7320266 -1.98774992
    ## 69  1 -2.2029899 -2.01834726
    ## 70  1 -2.1165003 -1.29612157
    ## 71  1 -0.8376995 -1.38512098
    ## 72  1 -3.9966399 -1.13264187
    ## 73  1 -3.7573714 -2.69820590
    ## 74  1 -1.1631932 -1.28420285
    ## 75  1 -2.7982095 -1.38786939
    ## 76  1 -4.0111124 -2.93587041
    ## 77  1 -0.8991398 -1.00972742
    ## 78  1 -1.8545135 -1.42775714
    ## 79  1 -1.4754846 -1.41243843
    ## 80  1 -1.2315441 -1.38844424
    ## 81  1 -2.5529982 -1.56449662
    ## 82  1 -1.2635707 -3.63267654
    ## 83  1 -1.8248306 -1.33195799
    ## 84  1 -1.9237411 -2.48899699
    ## 85  1 -1.8582736 -1.44456264
    ## 86  1 -2.6311394 -1.07385092
    ## 87  1 -1.1599336 -1.74598920
    ## 88  1 -3.3318293 -2.96657879
    ## 89  1 -2.7401863 -2.12472012
    ## 90  1 -4.0779608 -0.83578633
    ## 91  1 -2.0271342 -1.47309641
    ## 92  1 -0.8532070 -1.18445919
    ## 93  1 -2.2512083 -2.35824032
    ## 94  1 -2.1053917 -1.39623657
    ## 95  1 -4.3367190 -3.57747763
    ## 96  1 -1.9434225 -1.05322344
    ## 97  1 -0.4058477 -3.03589868
    ## 98  1 -0.6835187 -3.28161003
    ## 99  1 -2.3147746 -3.60645375
    ## 100 1 -2.9335825 -3.06177747

### Soft Margin SVM의 $C$ 값이 아주 큰 경우($\infty$)를 설정

```r
# Soft Margin with Large C
model_largec <- svm(as.factor(y)~.,kernel="linear",cost=10e15, data=dat)
summary(model_largec)
```

    ##
    ## Call:
    ## svm(formula = as.factor(y) ~ ., data = dat, kernel = "linear", cost = 1e+16)
    ##
    ##
    ## Parameters:
    ##    SVM-Type:  C-classification
    ##  SVM-Kernel:  linear
    ##        cost:  1e+16
    ##
    ## Number of Support Vectors:  3
    ##
    ##  ( 2 1 )
    ##
    ##
    ## Number of Classes:  2
    ##
    ## Levels:
    ##  0 1

```r
predict(model_largec,dat[,-1])
```

    ##   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20
    ##   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    ##  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40
    ##   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    ##  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60
    ##   0   0   0   0   0   0   0   0   0   0   1   1   1   1   1   1   1   1   1   1
    ##  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80
    ##   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1
    ##  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100
    ##   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1
    ## Levels: 0 1

### Soft Margin이지만 $C$가 아주 크게 되면 실제로 Hard Margin과 같아짐을 확인

### (=선형 분리됨)

```r
plot(model_largec,dat)
```

![](Hard-Margin-SVM-vs-Soft-Margin-SVM_files/figure-gfm/unnamed-chunk-4-1.png)<!-- -->

### 이때의 $w$ 값 확인(Support Vector가 존재함)

```r
model_largec$coefs
```

    ##            [,1]
    ## [1,]  414.69270
    ## [2,]   12.31121
    ## [3,] -427.00391

### 추가적으로 $c$가 매우 작은 경우를 설정

```r
model_smallc <- svm(as.factor(y)~.,kernel="linear",cost=10e-15, data=dat) # c가 없는경우(cost=10e-15)으로 큰 경우
summary(model_smallc)
```

    ##
    ## Call:
    ## svm(formula = as.factor(y) ~ ., data = dat, kernel = "linear", cost = 1e-14)
    ##
    ##
    ## Parameters:
    ##    SVM-Type:  C-classification
    ##  SVM-Kernel:  linear
    ##        cost:  1e-14
    ##
    ## Number of Support Vectors:  100
    ##
    ##  ( 50 50 )
    ##
    ##
    ## Number of Classes:  2
    ##
    ## Levels:
    ##  0 1

```r
predict(model_smallc,dat[,-1])
```

    ##   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20
    ##   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    ##  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40
    ##   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    ##  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60
    ##   0   0   0   0   0   0   0   0   0   0   1   1   1   0   1   1   1   1   1   1
    ##  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80
    ##   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1
    ##  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100
    ##   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1
    ## Levels: 0 1

### Soft Margin에서 $C$가 아주 작은 경우 확인

### (=선형 분리되는 것 처럼 보임)

```r
plot(model_smallc,dat)
```

![](Hard-Margin-SVM-vs-Soft-Margin-SVM_files/figure-gfm/unnamed-chunk-7-1.png)<!-- -->

### 이때의 $w$ 값 확인

### (모든 점이 support vector가 됨 = w가 전부 0 = 의미없음)

```r
model_smallc$coefs
```

    ##          [,1]
    ##   [1,]  1e-14
    ##   [2,]  1e-14
    ##   [3,]  1e-14
    ##   [4,]  1e-14
    ##   [5,]  1e-14
    ##   [6,]  1e-14
    ##   [7,]  1e-14
    ##   [8,]  1e-14
    ##   [9,]  1e-14
    ##  [10,]  1e-14
    ##  [11,]  1e-14
    ##  [12,]  1e-14
    ##  [13,]  1e-14
    ##  [14,]  1e-14
    ##  [15,]  1e-14
    ##  [16,]  1e-14
    ##  [17,]  1e-14
    ##  [18,]  1e-14
    ##  [19,]  1e-14
    ##  [20,]  1e-14
    ##  [21,]  1e-14
    ##  [22,]  1e-14
    ##  [23,]  1e-14
    ##  [24,]  1e-14
    ##  [25,]  1e-14
    ##  [26,]  1e-14
    ##  [27,]  1e-14
    ##  [28,]  1e-14
    ##  [29,]  1e-14
    ##  [30,]  1e-14
    ##  [31,]  1e-14
    ##  [32,]  1e-14
    ##  [33,]  1e-14
    ##  [34,]  1e-14
    ##  [35,]  1e-14
    ##  [36,]  1e-14
    ##  [37,]  1e-14
    ##  [38,]  1e-14
    ##  [39,]  1e-14
    ##  [40,]  1e-14
    ##  [41,]  1e-14
    ##  [42,]  1e-14
    ##  [43,]  1e-14
    ##  [44,]  1e-14
    ##  [45,]  1e-14
    ##  [46,]  1e-14
    ##  [47,]  1e-14
    ##  [48,]  1e-14
    ##  [49,]  1e-14
    ##  [50,]  1e-14
    ##  [51,] -1e-14
    ##  [52,] -1e-14
    ##  [53,] -1e-14
    ##  [54,] -1e-14
    ##  [55,] -1e-14
    ##  [56,] -1e-14
    ##  [57,] -1e-14
    ##  [58,] -1e-14
    ##  [59,] -1e-14
    ##  [60,] -1e-14
    ##  [61,] -1e-14
    ##  [62,] -1e-14
    ##  [63,] -1e-14
    ##  [64,] -1e-14
    ##  [65,] -1e-14
    ##  [66,] -1e-14
    ##  [67,] -1e-14
    ##  [68,] -1e-14
    ##  [69,] -1e-14
    ##  [70,] -1e-14
    ##  [71,] -1e-14
    ##  [72,] -1e-14
    ##  [73,] -1e-14
    ##  [74,] -1e-14
    ##  [75,] -1e-14
    ##  [76,] -1e-14
    ##  [77,] -1e-14
    ##  [78,] -1e-14
    ##  [79,] -1e-14
    ##  [80,] -1e-14
    ##  [81,] -1e-14
    ##  [82,] -1e-14
    ##  [83,] -1e-14
    ##  [84,] -1e-14
    ##  [85,] -1e-14
    ##  [86,] -1e-14
    ##  [87,] -1e-14
    ##  [88,] -1e-14
    ##  [89,] -1e-14
    ##  [90,] -1e-14
    ##  [91,] -1e-14
    ##  [92,] -1e-14
    ##  [93,] -1e-14
    ##  [94,] -1e-14
    ##  [95,] -1e-14
    ##  [96,] -1e-14
    ##  [97,] -1e-14
    ##  [98,] -1e-14
    ##  [99,] -1e-14
    ## [100,] -1e-14

### 결론

1.  Soft Margin SVM에서 $c$가 $\infinity$가 되면 Hard Margin SVM(SVM)과
    같다.
2.  Soft Margin SVM에서 $c$가 매우 작아지면 최적화 과정에서 error를
    고려하지 않기 때문에 가능하지 않다 (의미없다)
